{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import uniform\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare running configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = \"twitter\"\n",
    "topic = \"elections\"\n",
    "seed = 42\n",
    "\n",
    "dataset_filename = f\"./{topic}/{social}/trees/{social}_{topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "df_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the following features to each conversation:\n",
    "\n",
    "- Is the root toxic?\n",
    "- Distance (in seconds) from the last comment\n",
    "- Percentage of distinct users commenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the toxicity of the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the distance in seconds from the last comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each conversation, we extract the one pair of toxic and non toxic tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all-features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe : pd.DataFrame, feature_labels, target_label):\n",
    "    \n",
    "    X = dataframe[feature_labels].to_numpy()\n",
    "    \n",
    "    y = dataframe[target_label].to_numpy()\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels =  [\n",
    " 'tree_size',\n",
    " 'max_width',\n",
    " 'max_depth',\n",
    " 'number_of_unique_users',\n",
    " 'toxicity_ratio',\n",
    " 'assortativity',\n",
    " 'avg_toxicity_distance',\n",
    " 'wiener_index',\n",
    " 'is_root_toxic',\n",
    " 'last_comments_diff_seconds']\n",
    "\n",
    "target_label = \"is_toxic\"\n",
    "\n",
    "evaluation_metrics = [\"accuracy\", \"roc_auc\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "number_of_folds = 10\n",
    "seed = 42\n",
    "\n",
    "RF_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "    (\"std\", StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    (\"clf\", RandomForestClassifier(random_state = seed))\n",
    "])\n",
    "\n",
    "RF_stratified_k_fold = StratifiedKFold(n_splits = number_of_folds, \n",
    "                                       shuffle = True, \n",
    "                                       random_state = seed)\n",
    "\n",
    "RF_grid = {\n",
    "\n",
    "    \"clf__n_estimators\": [10, 50, 100, 1000],\n",
    "    # 'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'clf__criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "RF_CV = GridSearchCV(\n",
    "    estimator=RF_pipe, \n",
    "    param_grid=RF_grid, \n",
    "    cv=RF_stratified_k_fold,\n",
    "    scoring= \"accuracy\",\n",
    "    refit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pairs of toxic/non toxic nodes from each conversation in the three bins\n",
    "Moreover, we will be careful to cover the entire children index spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "\n",
    "print(f\"{social} {topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Train size: \" + str(X_train.shape[0]))\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    RF_CV = RF_CV.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = RF_CV.predict(X_test)\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "print(\"Train size: \" + str(X_train.shape[0]))\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "RF_CV = RF_CV.fit(X_train, Y_train)\n",
    "Y_pred = RF_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
