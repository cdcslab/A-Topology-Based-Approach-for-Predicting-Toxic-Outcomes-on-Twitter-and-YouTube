{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import uniform\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare running configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = \"twitter\"\n",
    "topic = \"elections\"\n",
    "seed = 42\n",
    "\n",
    "dataset_filename = f\"/media/gabett/Volume/data-repository/panconesi-football-elections/{topic}/{social}/trees/{social}_{topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2742491, 18)\n",
      "children_index                          int32\n",
      "conversation_id                       float64\n",
      "id                                     object\n",
      "parent_id                              object\n",
      "created_at                datetime64[ns, UTC]\n",
      "root                                   object\n",
      "toxicity_score                        float64\n",
      "tree_size                             float64\n",
      "max_width                             float64\n",
      "max_depth                             float64\n",
      "number_of_unique_users                float64\n",
      "toxicity_ratio                        float64\n",
      "assortativity                         float64\n",
      "avg_toxicity_distance                 float64\n",
      "wiener_index                          float64\n",
      "is_toxic                               object\n",
      "social                                 object\n",
      "topic                                  object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children_index</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>root</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>tree_size</th>\n",
       "      <th>max_width</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>number_of_unique_users</th>\n",
       "      <th>toxicity_ratio</th>\n",
       "      <th>assortativity</th>\n",
       "      <th>avg_toxicity_distance</th>\n",
       "      <th>wiener_index</th>\n",
       "      <th>is_toxic</th>\n",
       "      <th>social</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.562687e+18</td>\n",
       "      <td>1562762800258490368</td>\n",
       "      <td>1.562686650136e+18</td>\n",
       "      <td>2022-08-25 11:24:20+00:00</td>\n",
       "      <td>1.562686650136e+18</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.562689e+18</td>\n",
       "      <td>1562690480013332480</td>\n",
       "      <td>1.562688726698e+18</td>\n",
       "      <td>2022-08-25 06:36:58+00:00</td>\n",
       "      <td>1.562688726698e+18</td>\n",
       "      <td>0.152140</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.562689e+18</td>\n",
       "      <td>1562809876560359424</td>\n",
       "      <td>1.562688726698e+18</td>\n",
       "      <td>2022-08-25 14:31:24+00:00</td>\n",
       "      <td>1.562688726698e+18</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.562710e+18</td>\n",
       "      <td>1562837692114960384</td>\n",
       "      <td>1.562710177308e+18</td>\n",
       "      <td>2022-08-25 16:21:56+00:00</td>\n",
       "      <td>1.562710177308e+18</td>\n",
       "      <td>0.012692</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.562710e+18</td>\n",
       "      <td>1563047853572128768</td>\n",
       "      <td>1.562710177308e+18</td>\n",
       "      <td>2022-08-26 06:17:02+00:00</td>\n",
       "      <td>1.562710177308e+18</td>\n",
       "      <td>0.132146</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children_index  conversation_id                   id           parent_id  \\\n",
       "0               1     1.562687e+18  1562762800258490368  1.562686650136e+18   \n",
       "1               1     1.562689e+18  1562690480013332480  1.562688726698e+18   \n",
       "2               2     1.562689e+18  1562809876560359424  1.562688726698e+18   \n",
       "3               1     1.562710e+18  1562837692114960384  1.562710177308e+18   \n",
       "4               2     1.562710e+18  1563047853572128768  1.562710177308e+18   \n",
       "\n",
       "                 created_at                root  toxicity_score  tree_size  \\\n",
       "0 2022-08-25 11:24:20+00:00  1.562686650136e+18        0.099415        2.0   \n",
       "1 2022-08-25 06:36:58+00:00  1.562688726698e+18        0.152140        2.0   \n",
       "2 2022-08-25 14:31:24+00:00  1.562688726698e+18        0.058985        3.0   \n",
       "3 2022-08-25 16:21:56+00:00  1.562710177308e+18        0.012692        2.0   \n",
       "4 2022-08-26 06:17:02+00:00  1.562710177308e+18        0.132146        3.0   \n",
       "\n",
       "   max_width  max_depth  number_of_unique_users  toxicity_ratio  \\\n",
       "0        1.0        1.0                     2.0             0.0   \n",
       "1        1.0        1.0                     2.0             0.0   \n",
       "2        2.0        1.0                     3.0             0.0   \n",
       "3        1.0        1.0                     2.0             0.0   \n",
       "4        2.0        1.0                     3.0             0.0   \n",
       "\n",
       "   assortativity  avg_toxicity_distance  wiener_index is_toxic   social  \\\n",
       "0            NaN                    NaN           1.0    False  twitter   \n",
       "1            NaN                    NaN           1.0    False  twitter   \n",
       "2            NaN                    NaN           1.0    False  twitter   \n",
       "3            NaN                    NaN           1.0    False  twitter   \n",
       "4            NaN                    NaN           1.0    False  twitter   \n",
       "\n",
       "       topic  \n",
       "0  elections  \n",
       "1  elections  \n",
       "2  elections  \n",
       "3  elections  \n",
       "4  elections  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "df_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the following features to each conversation:\n",
    "\n",
    "- Is the root toxic?\n",
    "- Distance (in seconds) from the last comment\n",
    "- Percentage of distinct users commenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the toxicity of the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47845/2371856118.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n"
     ]
    }
   ],
   "source": [
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the distance in seconds from the last comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each conversation, we extract the one pair of toxic and non toxic tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     105134\n",
       "False    105134\n",
       "Name: is_toxic, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all-features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe : pd.DataFrame, feature_labels, target_label):\n",
    "    \n",
    "    X = dataframe[feature_labels].to_numpy()\n",
    "    \n",
    "    y = dataframe[target_label].to_numpy()\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels =  [\n",
    " 'tree_size',\n",
    " 'max_width',\n",
    " 'max_depth',\n",
    " 'number_of_unique_users',\n",
    " 'toxicity_ratio',\n",
    " 'assortativity',\n",
    " 'avg_toxicity_distance',\n",
    " 'wiener_index',\n",
    " 'is_root_toxic',\n",
    " 'last_comments_diff_seconds']\n",
    "\n",
    "target_label = \"is_toxic\"\n",
    "\n",
    "evaluation_metrics = [\"accuracy\", \"roc_auc\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "number_of_folds = 10\n",
    "seed = 42\n",
    "\n",
    "DT_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "    (\"std\", StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state = seed))\n",
    "])\n",
    "\n",
    "DT_stratified_k_fold = StratifiedKFold(n_splits = number_of_folds, \n",
    "                                       shuffle = True, \n",
    "                                       random_state = seed)\n",
    "\n",
    "DT_grid = {\n",
    "    'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'clf__max_depth' : [10, 50, 100, 500, 1000, 2000, 3000, 5000, 10000],\n",
    "    'clf__criterion' :['gini', 'entropy']        \n",
    "}\n",
    "\n",
    "DT_CV = GridSearchCV(\n",
    "    estimator=DT_pipe, \n",
    "    param_grid=DT_grid, \n",
    "    cv=DT_stratified_k_fold,\n",
    "    scoring= \"accuracy\",\n",
    "    refit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pairs of toxic/non toxic nodes from each conversation in the three bins\n",
    "Moreover, we will be careful to cover the entire children index spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter elections Classification Results\n",
      "Bin [10, 100]\n",
      "Train size: 58079\n",
      "Test size: 14520\n",
      "\n",
      "Test Accuracy: 0.777066\n",
      "Test Precision: 0.710145\n",
      "Test Recall: 0.899277\n",
      "Test F1: 0.793598\n",
      "Test AUC: 0.782533\n",
      "\n",
      "Bin (100, 1000]\n",
      "Train size: 67435\n",
      "Test size: 16859\n",
      "\n",
      "Test Accuracy: 0.654072\n",
      "Test Precision: 0.655267\n",
      "Test Recall: 0.790845\n",
      "Test F1: 0.716701\n",
      "Test AUC: 0.637754\n",
      "\n",
      "Bin (1000, 10000]\n",
      "Train size: 12379\n",
      "Test size: 3095\n",
      "\n",
      "Test Accuracy: 0.671729\n",
      "Test Precision: 0.684526\n",
      "Test Recall: 0.882929\n",
      "Test F1: 0.771171\n",
      "Test AUC: 0.600202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "\n",
    "print(f\"{social} {topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Train size: \" + str(X_train.shape[0]))\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    DT_CV = DT_CV.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = DT_CV.predict(X_test)\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 168214\n",
      "Test size: 42054\n",
      "\n",
      "Test Accuracy: 0.749536\n",
      "Test Precision: 0.698660\n",
      "Test Recall: 0.877586\n",
      "Test F1: 0.777967\n",
      "Test AUC: 0.749536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "print(\"Train size: \" + str(X_train.shape[0]))\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "DT_CV = DT_CV.fit(X_train, Y_train)\n",
    "Y_pred = DT_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
