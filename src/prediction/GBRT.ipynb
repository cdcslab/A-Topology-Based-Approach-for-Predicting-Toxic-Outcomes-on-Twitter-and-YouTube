{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import uniform\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare running configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = \"twitter\"\n",
    "topic = \"football\"\n",
    "adversarial_topic = \"elections\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "dataset_filename = f\"./{topic}/{social}/trees/{social}_{topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "df_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the following features to each conversation:\n",
    "\n",
    "- Is the root toxic?\n",
    "- Distance (in seconds) from the last comment\n",
    "- Percentage of distinct users commenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the toxicity of the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the distance in seconds from the last comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each conversation, we extract the one pair of toxic and non toxic tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all-features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe : pd.DataFrame, feature_labels, target_label):\n",
    "    \n",
    "    X = dataframe[feature_labels].to_numpy()\n",
    "    \n",
    "    y = dataframe[target_label].to_numpy()\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels =  [\n",
    " 'tree_size',\n",
    " 'max_width',\n",
    " 'max_depth',\n",
    " 'number_of_unique_users',\n",
    " 'toxicity_ratio',\n",
    " 'assortativity',\n",
    " 'avg_toxicity_distance',\n",
    " 'wiener_index',\n",
    " 'is_root_toxic',\n",
    " 'last_comments_diff_seconds']\n",
    "\n",
    "target_label = \"is_toxic\"\n",
    "\n",
    "evaluation_metrics = [\"accuracy\", \"roc_auc\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "number_of_folds = 10\n",
    "seed = 42\n",
    "\n",
    "GBRT_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "    (\"std\", StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    (\"clf\", LGBMClassifier(random_state = seed))\n",
    "])\n",
    "\n",
    "GBRT_stratified_k_fold = StratifiedKFold(n_splits = number_of_folds, \n",
    "                                       shuffle = True, \n",
    "                                       random_state = seed)\n",
    "\n",
    "GBRT_grid = {\n",
    "    \"clf__n_estimators\": [10, 50, 100, 1000]\n",
    "}\n",
    "\n",
    "GBRT_CV = GridSearchCV(\n",
    "    estimator=GBRT_pipe, \n",
    "    param_grid=GBRT_grid, \n",
    "    cv=GBRT_stratified_k_fold,\n",
    "    scoring= \"accuracy\",\n",
    "    refit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pairs of toxic/non toxic nodes from each conversation in the three bins\n",
    "Moreover, we will be careful to cover the entire children index spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "df_complete_importances = pd.DataFrame()\n",
    "print(f\"{social} {topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Train size: \" + str(X_train.shape[0]))\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    GBRT_CV = GBRT_CV.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "\n",
    "    feature_importance = GBRT_CV.best_estimator_[\"clf\"].feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    result = permutation_importance(\n",
    "        GBRT_CV, X_test, Y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    labels = np.array(feature_labels)[sorted_idx]\n",
    "\n",
    "    df_bin_importance = pd.DataFrame(result.importances[sorted_idx].T, columns = labels)\n",
    "    df_bin_importance[\"social\"] = social\n",
    "    df_bin_importance[\"interval\"] = f\"Bin [{min_bin}, {max_bin}]\"\n",
    "    \n",
    "    df_complete_importances = pd.concat([df_complete_importances, df_bin_importance])\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "print(\"Train size: \" + str(X_train.shape[0]))\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "GBRT_CV = GBRT_CV.fit(X_train, Y_train)\n",
    "Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "feature_importance = GBRT_CV.best_estimator_[\"clf\"].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "result = permutation_importance(\n",
    "    GBRT_CV, X_test, Y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "labels = np.array(feature_labels)[sorted_idx]\n",
    "\n",
    "df_bin_importance = pd.DataFrame(result.importances[sorted_idx].T, columns = labels)\n",
    "df_bin_importance[\"social\"] = social\n",
    "df_bin_importance[\"interval\"] = \"overall\"\n",
    "df_complete_importances = pd.concat([df_complete_importances, df_bin_importance])\n",
    "\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"./ML/{social}_{topic}_feature_importances.csv\"\n",
    "df_complete_importances.to_csv(output_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing against other topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = f\"./{adversarial_topic}/{social}/trees/{social}_{adversarial_topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'\n",
    "\n",
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "\n",
    "df_data.head()\n",
    "\n",
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))\n",
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "\n",
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against adversarial topic test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "\n",
    "print(f\"{social} {adversarial_topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
