{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import uniform\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare running configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = \"twitter\"\n",
    "topic = \"football\"\n",
    "adversarial_topic = \"elections\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "dataset_filename = f\"/media/gabett/Volume/data-repository/panconesi-football-elections/{topic}/{social}/trees/{social}_{topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(930129, 18)\n",
      "children_index                          int32\n",
      "conversation_id                       float64\n",
      "id                                     object\n",
      "parent_id                              object\n",
      "created_at                datetime64[ns, UTC]\n",
      "root                                   object\n",
      "toxicity_score                        float64\n",
      "tree_size                             float64\n",
      "max_width                             float64\n",
      "max_depth                             float64\n",
      "number_of_unique_users                float64\n",
      "toxicity_ratio                        float64\n",
      "assortativity                         float64\n",
      "avg_toxicity_distance                 float64\n",
      "wiener_index                          float64\n",
      "is_toxic                               object\n",
      "social                                 object\n",
      "topic                                  object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children_index</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>root</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>tree_size</th>\n",
       "      <th>max_width</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>number_of_unique_users</th>\n",
       "      <th>toxicity_ratio</th>\n",
       "      <th>assortativity</th>\n",
       "      <th>avg_toxicity_distance</th>\n",
       "      <th>wiener_index</th>\n",
       "      <th>is_toxic</th>\n",
       "      <th>social</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.563036e+18</td>\n",
       "      <td>1563037932386078720</td>\n",
       "      <td>1.563036027912e+18</td>\n",
       "      <td>2022-08-26 05:37:37+00:00</td>\n",
       "      <td>1.563036027912e+18</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.563077e+18</td>\n",
       "      <td>1563077671764250624</td>\n",
       "      <td>1.563076923966e+18</td>\n",
       "      <td>2022-08-26 08:15:32+00:00</td>\n",
       "      <td>1.563076923966e+18</td>\n",
       "      <td>0.008733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.563077e+18</td>\n",
       "      <td>1563082289977835520</td>\n",
       "      <td>1.563076923966e+18</td>\n",
       "      <td>2022-08-26 08:33:53+00:00</td>\n",
       "      <td>1.563076923966e+18</td>\n",
       "      <td>0.225790</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.563083e+18</td>\n",
       "      <td>1563084637131251712</td>\n",
       "      <td>1.563082790375e+18</td>\n",
       "      <td>2022-08-26 08:43:12+00:00</td>\n",
       "      <td>1.563082790375e+18</td>\n",
       "      <td>0.211073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.563146e+18</td>\n",
       "      <td>1563146712042323968</td>\n",
       "      <td>1.56314636959e+18</td>\n",
       "      <td>2022-08-26 12:49:52+00:00</td>\n",
       "      <td>1.56314636959e+18</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>twitter</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children_index  conversation_id                   id           parent_id  \\\n",
       "0               1     1.563036e+18  1563037932386078720  1.563036027912e+18   \n",
       "1               1     1.563077e+18  1563077671764250624  1.563076923966e+18   \n",
       "2               2     1.563077e+18  1563082289977835520  1.563076923966e+18   \n",
       "3               1     1.563083e+18  1563084637131251712  1.563082790375e+18   \n",
       "4               1     1.563146e+18  1563146712042323968   1.56314636959e+18   \n",
       "\n",
       "                 created_at                root  toxicity_score  tree_size  \\\n",
       "0 2022-08-26 05:37:37+00:00  1.563036027912e+18        0.004869        2.0   \n",
       "1 2022-08-26 08:15:32+00:00  1.563076923966e+18        0.008733        2.0   \n",
       "2 2022-08-26 08:33:53+00:00  1.563076923966e+18        0.225790        3.0   \n",
       "3 2022-08-26 08:43:12+00:00  1.563082790375e+18        0.211073        2.0   \n",
       "4 2022-08-26 12:49:52+00:00   1.56314636959e+18        0.058985        2.0   \n",
       "\n",
       "   max_width  max_depth  number_of_unique_users  toxicity_ratio  \\\n",
       "0        1.0        1.0                     2.0             0.0   \n",
       "1        1.0        1.0                     2.0             0.0   \n",
       "2        2.0        1.0                     3.0             0.0   \n",
       "3        1.0        1.0                     2.0             0.0   \n",
       "4        1.0        1.0                     2.0             0.0   \n",
       "\n",
       "   assortativity  avg_toxicity_distance  wiener_index is_toxic   social  \\\n",
       "0            NaN                    NaN           1.0    False  twitter   \n",
       "1            NaN                    NaN           1.0    False  twitter   \n",
       "2            NaN                    NaN           1.0    False  twitter   \n",
       "3            NaN                    NaN           1.0    False  twitter   \n",
       "4            NaN                    NaN           1.0    False  twitter   \n",
       "\n",
       "      topic  \n",
       "0  football  \n",
       "1  football  \n",
       "2  football  \n",
       "3  football  \n",
       "4  football  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "df_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the following features to each conversation:\n",
    "\n",
    "- Is the root toxic?\n",
    "- Distance (in seconds) from the last comment\n",
    "- Percentage of distinct users commenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the toxicity of the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_796554/2371856118.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n"
     ]
    }
   ],
   "source": [
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the feature regarding the distance in seconds from the last comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each conversation, we extract the one pair of toxic and non toxic tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     26356\n",
       "False    26356\n",
       "Name: is_toxic, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all-features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe : pd.DataFrame, feature_labels, target_label):\n",
    "    \n",
    "    X = dataframe[feature_labels].to_numpy()\n",
    "    \n",
    "    y = dataframe[target_label].to_numpy()\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels =  [\n",
    " 'tree_size',\n",
    " 'max_width',\n",
    " 'max_depth',\n",
    " 'number_of_unique_users',\n",
    " 'toxicity_ratio',\n",
    " 'assortativity',\n",
    " 'avg_toxicity_distance',\n",
    " 'wiener_index',\n",
    " 'is_root_toxic',\n",
    " 'last_comments_diff_seconds']\n",
    "\n",
    "target_label = \"is_toxic\"\n",
    "\n",
    "evaluation_metrics = [\"accuracy\", \"roc_auc\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "number_of_folds = 10\n",
    "seed = 42\n",
    "\n",
    "GBRT_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "    (\"std\", StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
    "    (\"clf\", LGBMClassifier(random_state = seed))\n",
    "])\n",
    "\n",
    "GBRT_stratified_k_fold = StratifiedKFold(n_splits = number_of_folds, \n",
    "                                       shuffle = True, \n",
    "                                       random_state = seed)\n",
    "\n",
    "GBRT_grid = {\n",
    "    \"clf__n_estimators\": [10, 50, 100, 1000]\n",
    "}\n",
    "\n",
    "GBRT_CV = GridSearchCV(\n",
    "    estimator=GBRT_pipe, \n",
    "    param_grid=GBRT_grid, \n",
    "    cv=GBRT_stratified_k_fold,\n",
    "    scoring= \"accuracy\",\n",
    "    refit=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract pairs of toxic/non toxic nodes from each conversation in the three bins\n",
    "Moreover, we will be careful to cover the entire children index spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter football Classification Results\n",
      "Bin [10, 100]\n",
      "Train size: 17201\n",
      "Test size: 4301\n",
      "\n",
      "Test Accuracy: 0.848175\n",
      "Test Precision: 0.776016\n",
      "Test Recall: 0.949279\n",
      "Test F1: 0.853948\n",
      "Test AUC: 0.854334\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8400679702174235\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n",
      "Bin (100, 1000]\n",
      "Train size: 12801\n",
      "Test size: 3201\n",
      "\n",
      "Test Accuracy: 0.728835\n",
      "Test Precision: 0.725869\n",
      "Test Recall: 0.880791\n",
      "Test F1: 0.795861\n",
      "Test AUC: 0.690786\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7208036324160811\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 100}\n",
      "Bin (1000, 10000]\n",
      "Train size: 2128\n",
      "Test size: 532\n",
      "\n",
      "Test Accuracy: 0.704887\n",
      "Test Precision: 0.745413\n",
      "Test Recall: 0.876011\n",
      "Test F1: 0.805452\n",
      "Test AUC: 0.593285\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.7293139339179732\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "df_complete_importances = pd.DataFrame()\n",
    "print(f\"{social} {topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Train size: \" + str(X_train.shape[0]))\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    GBRT_CV = GBRT_CV.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "\n",
    "    feature_importance = GBRT_CV.best_estimator_[\"clf\"].feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    result = permutation_importance(\n",
    "        GBRT_CV, X_test, Y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    labels = np.array(feature_labels)[sorted_idx]\n",
    "\n",
    "    df_bin_importance = pd.DataFrame(result.importances[sorted_idx].T, columns = labels)\n",
    "    df_bin_importance[\"social\"] = social\n",
    "    df_bin_importance[\"interval\"] = f\"Bin [{min_bin}, {max_bin}]\"\n",
    "    \n",
    "    df_complete_importances = pd.concat([df_complete_importances, df_bin_importance])\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 42169\n",
      "Test size: 10543\n",
      "\n",
      "Test Accuracy: 0.836669\n",
      "Test Precision: 0.781087\n",
      "Test Recall: 0.935496\n",
      "Test F1: 0.851347\n",
      "Test AUC: 0.836678\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8331239293471487\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "print(\"Train size: \" + str(X_train.shape[0]))\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "GBRT_CV = GBRT_CV.fit(X_train, Y_train)\n",
    "Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "feature_importance = GBRT_CV.best_estimator_[\"clf\"].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "result = permutation_importance(\n",
    "    GBRT_CV, X_test, Y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "labels = np.array(feature_labels)[sorted_idx]\n",
    "\n",
    "df_bin_importance = pd.DataFrame(result.importances[sorted_idx].T, columns = labels)\n",
    "df_bin_importance[\"social\"] = social\n",
    "df_bin_importance[\"interval\"] = \"overall\"\n",
    "df_complete_importances = pd.concat([df_complete_importances, df_bin_importance])\n",
    "\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"/media/gabett/Volume/data-repository/panconesi-football-elections/ML/{social}_{topic}_feature_importances.csv\"\n",
    "df_complete_importances.to_csv(output_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing against other topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adversarial_topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gabett/Documents/repository/football-elections-cascade-comparison/src/prediction/GBRT.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gabett/Documents/repository/football-elections-cascade-comparison/src/prediction/GBRT.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/media/gabett/Volume/data-repository/panconesi-football-elections/\u001b[39m\u001b[39m{\u001b[39;00madversarial_topic\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msocial\u001b[39m}\u001b[39;00m\u001b[39m/trees/\u001b[39m\u001b[39m{\u001b[39;00msocial\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00madversarial_topic\u001b[39m}\u001b[39;00m\u001b[39m_all_graphs_unified.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gabett/Documents/repository/football-elections-cascade-comparison/src/prediction/GBRT.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m social \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39myoutube\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gabett/Documents/repository/football-elections-cascade-comparison/src/prediction/GBRT.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     thread_identifier \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvideo_id\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adversarial_topic' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_filename = f\"/media/gabett/Volume/data-repository/panconesi-football-elections/{adversarial_topic}/{social}/trees/{social}_{adversarial_topic}_all_graphs_unified.parquet\"\n",
    "\n",
    "if social == \"youtube\":\n",
    "    thread_identifier = 'video_id'\n",
    "else:\n",
    "    thread_identifier = 'conversation_id'\n",
    "\n",
    "df_data = pd.read_parquet(dataset_filename)\n",
    "\n",
    "# We remove all rows without a toxic label\n",
    "df_data = df_data[df_data['is_toxic'].notna()]\n",
    "print(df_data.shape)\n",
    "print(df_data.dtypes)\n",
    "\n",
    "df_data.head()\n",
    "\n",
    "# Is the root toxic?\n",
    "thread_roots = df_data[df_data.children_index == 1]\n",
    "thread_roots.loc[:, \"is_root_toxic\"] = thread_roots.toxicity_score > 0.6\n",
    "thread_roots = thread_roots[[thread_identifier, \"is_root_toxic\"]]\n",
    "\n",
    "df_data = df_data.merge(thread_roots, 'inner', left_on= thread_identifier, right_on = thread_identifier,  suffixes= (None, \"_y\"))\n",
    "df_data['created_at'] = pd.to_datetime(df_data['created_at'])\n",
    "df_data['last_comments_diff_seconds'] = df_data.groupby(thread_identifier)['created_at'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "df_paired_tweets = pd.DataFrame({}, \n",
    "                                columns=df_data.columns)\n",
    "\n",
    "df_data_to_evaluate = df_data[(df_data[\"toxicity_score\"] <= 0.2) | (df_data[\"toxicity_score\"] >= 0.6)]\n",
    "number_of_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True].shape[0]\n",
    "number_of_non_toxic_tweets = df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False].shape[0]\n",
    "\n",
    "downsample_size = min(number_of_non_toxic_tweets, number_of_toxic_tweets)\n",
    "minority_class = np.argmin([number_of_non_toxic_tweets, number_of_toxic_tweets])\n",
    "\n",
    "if minority_class == 0: # non toxic\n",
    "    df_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True],\n",
    "                                         n_samples = number_of_non_toxic_tweets,\n",
    "                                         random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets,\n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False], df_toxic_tweets_resampled], ignore_index=True)],\n",
    "                                  ignore_index=True)\n",
    "else:\n",
    "    df_non_toxic_tweets_resampled = resample(df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == False],\n",
    "                                             n_samples = number_of_toxic_tweets,\n",
    "                                             random_state=seed)\n",
    "    \n",
    "    \n",
    "    df_paired_tweets = pd.concat([df_paired_tweets, \n",
    "                                  pd.concat([df_data_to_evaluate[df_data_to_evaluate['is_toxic'] == True], df_non_toxic_tweets_resampled], ignore_index=True)], \n",
    "                                  ignore_index=True)\n",
    "    \n",
    "df_paired_tweets.is_toxic.value_counts()\n",
    "\n",
    "df_bin_10_100 = df_paired_tweets.query(\"children_index >= 10 & children_index <= 100\")\n",
    "df_bin_100_1000 = df_paired_tweets.query(\"children_index > 100 & children_index <= 1000\")\n",
    "df_bin_1000_10000 = df_paired_tweets.query(\"children_index > 1000 & children_index <= 10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against adversarial topic test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube elections Classification Results\n",
      "Bin [10, 100]\n",
      "Test size: 13596\n",
      "\n",
      "Test Accuracy: 0.729185\n",
      "Test Precision: 0.634314\n",
      "Test Recall: 0.981913\n",
      "Test F1: 0.770735\n",
      "Test AUC: 0.746339\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8122439758723367\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n",
      "Bin (100, 1000]\n",
      "Test size: 22253\n",
      "\n",
      "Test Accuracy: 0.626343\n",
      "Test Precision: 0.575355\n",
      "Test Recall: 0.965043\n",
      "Test F1: 0.720908\n",
      "Test AUC: 0.626297\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8122439758723367\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n",
      "Bin (1000, 10000]\n",
      "Test size: 5659\n",
      "\n",
      "Test Accuracy: 0.641456\n",
      "Test Precision: 0.633531\n",
      "Test Recall: 0.984722\n",
      "Test F1: 0.771019\n",
      "Test AUC: 0.541219\n",
      "\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8122439758723367\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "bins = [df_bin_10_100, df_bin_100_1000, df_bin_1000_10000]\n",
    "# bins_to_test = [bin_10_100_test, bin_100_1000_test, bin_1000_10000_test]\n",
    "min_bin = 10\n",
    "\n",
    "\n",
    "print(f\"{social} {adversarial_topic} Classification Results\")\n",
    "for bin in bins:\n",
    "    i = 0\n",
    "    max_bin = min_bin * 10\n",
    "\n",
    "    if min_bin == 10:\n",
    "        print(f\"Bin [{min_bin}, {max_bin}]\")\n",
    "    else:\n",
    "        print(f\"Bin ({min_bin}, {max_bin}]\")   \n",
    "    \n",
    "    X, Y = create_dataset(bin, feature_labels, target_label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                        random_state=seed, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        shuffle = True,\n",
    "                                                        stratify=Y)\n",
    "\n",
    "    print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "    Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "    print()\n",
    "    print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "    print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "    print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "    print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "    print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "    confusion_matrix(Y_test, Y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "    min_bin *= 10\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 44594\n",
      "\n",
      "Test Accuracy: 0.681302\n",
      "Test Precision: 0.614301\n",
      "Test Recall: 0.974391\n",
      "Test F1: 0.753538\n",
      "Test AUC: 0.681302\n",
      " Results from Grid Search \n",
      "\n",
      " The best estimator across ALL searched params:\n",
      " Pipeline(steps=[('imputer', SimpleImputer()), ('std', StandardScaler()),\n",
      "                ('clf', LGBMClassifier(n_estimators=50, random_state=42))])\n",
      "\n",
      " The best score across ALL searched params:\n",
      " 0.8122439758723367\n",
      "\n",
      " The best parameters across ALL searched params:\n",
      " {'clf__n_estimators': 50}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_dataset(df_paired_tweets, feature_labels, target_label)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    random_state=seed, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle = True,\n",
    "                                                    stratify=Y)\n",
    "\n",
    "print(\"Test size: \" + str(X_test.shape[0]))\n",
    "\n",
    "Y_pred = GBRT_CV.predict(X_test)\n",
    "\n",
    "print()\n",
    "print('Test Accuracy: %3f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Test Precision: %3f' % precision_score(Y_test, Y_pred))\n",
    "print('Test Recall: %3f' % recall_score(Y_test, Y_pred))\n",
    "print('Test F1: %3f' % f1_score(Y_test, Y_pred))\n",
    "print('Test AUC: %3f' % roc_auc_score(Y_test, Y_pred))\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",GBRT_CV.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",GBRT_CV.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",GBRT_CV.best_params_)\n",
    "\n",
    "confusion_matrix(Y_test, Y_pred)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
